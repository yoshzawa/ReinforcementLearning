{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # ポップアウトウィンドウを表示しないようにする\n",
    "from ple import PLE\n",
    "from ple.games.flappybird import FlappyBird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \n",
    "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=16)\n",
    "    \n",
    "    anim.save('movie_flappy_bird_DQN.mp4')\n",
    "    display(display_animation(anim, default_mode='once'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_graph(reward_per_epoch, lifetime_per_epoch):\n",
    "    fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "    axL.set_title('lifetime')\n",
    "    axL.grid(True)\n",
    "    axL.plot(lifetime_per_epoch, color='magenta')\n",
    "    axR.set_title('reward')\n",
    "    axR.grid(True)\n",
    "    axR.plot(reward_per_epoch , color='magenta')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transicion', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99 # 時間割引率\n",
    "MAX_STEPS = 1200 # 1試行のstep数(フレーム数)\n",
    "NUM_EPISODES = 800 # 最大試行回数\n",
    "PRINT_EVERY_EPISODE = 50\n",
    "SHOW_GIF_EVERY_EPISODE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY #メモリの最大値\n",
    "        self.memory = [] # 経験を保存するリスト\n",
    "        self.index = 0 # 保存するindexを表す変数\n",
    "    \n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''trasicion = (state, action, state_next, reward)をメモリ保存する'''\n",
    "        \n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None) # メモリが満タンじゃないときは足す\n",
    "            \n",
    "        # namedtupleのTransitionを用意し、値とフィールド名をペアにする\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "            \n",
    "        self.index = (self.index + 1) % self.capacity # 保存するindexを1つずらす\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        '''batch_sizeだけ、ランダムに取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在のmemoryの長さを返す'''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_states, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        output = self.fc3(h2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions # 行動の数を取得\n",
    "        \n",
    "        # 経験を保存するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "        \n",
    "        # NNを構築\n",
    "        self.model = Net(num_states, num_actions)\n",
    "        \n",
    "        print(self.model) # ネットワークの形を出力\n",
    "        \n",
    "        # target_net\n",
    "        self.target_net = copy.deepcopy(self.model)\n",
    "        self.target_net.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        \n",
    "    def replay(self):\n",
    "        '''Experience Replayでネットワークの結合パラメータを出力'''\n",
    "        \n",
    "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        \n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "        # trainsicionsは1stepごとの(state, action. state_next, reward)が、BATCH_SIZE分格納されている\n",
    "        # つまり、(state, action, state_next, reward)xBATCH_SIZE\n",
    "        # これをミニバッチにしたい\n",
    "        # (state x BATCH_SIZE, action x BATCH_SIZE, state_next x BATCH_SIZE, reward x BATCH_SIZE)にする\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # 2.3  各変数の要素をミニバッチに対応する形に変形する\n",
    "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるが、\n",
    "        # それを torch.FloatTensor of BATCH_SIZE x 4に変換する\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        \n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "        # [torch.FloatTensor of size BATCH_SIZE x 2]になっている\n",
    "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが\n",
    "        # 右か左かのindexを求め、それに対応するQ値をgatherで引っ張り出す\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし、次の状態があるかに注意。\n",
    "        \n",
    "        # flappybirdがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "        \n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "        \n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求める\n",
    "        # そしてそのQ値を取り出します\n",
    "        self.target_net.eval()\n",
    "        next_state_values[non_final_mask] = self.target_net(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "        \n",
    "        # 3.4 教師となるQ値を、Q学習の式から求める\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "        \n",
    "        # ------------------------------------------------------------\n",
    "        # 4. 結合パラメータの更新　\n",
    "        # ------------------------------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "        \n",
    "        # 4.2 損失関数を計算する (smooth_l1_lossはHuberloss)\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているから、unsqueezeで[minbatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad() # 勾配をリセット\n",
    "        loss.backward() # バックプロパゲーションを計算\n",
    "        self.optimizer.step() # 結合パラメータを更新\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # モデルの重みをtarget_networkにコピー\n",
    "        self.target_net.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        epsilon = 0.41 * (1 / (episode + 1))\n",
    "        \n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出す = max(1)[1]\n",
    "            # .view(1, 1)は[torch.LongTensor of size 1] を size 1x1 に変換する\n",
    "        \n",
    "        else:\n",
    "            # 0, 1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                    [[random.randrange(self.num_actions)]])\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になる\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def brain_predict(self, state):\n",
    "        self.model.eval() # ネットワークを推論モードに切り替える\n",
    "        with torch.no_grad():\n",
    "            action = self.model(state).max(1)[1].view(1, 1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions)\n",
    "        # エージェントが行動を決定するための頭脳を生成\n",
    "        \n",
    "    def update_q_network(self):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay()\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.brain.update_target_model()\n",
    "        \n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "    \n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)\n",
    "    \n",
    "    def predict_action(self, state):\n",
    "        action = self.brain.brain_predict(state)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.game = FlappyBird()\n",
    "        self.env = PLE(self.game, fps=30, display_screen=False)\n",
    "        self.num_states = len(self.game.getGameState())  # 8\n",
    "        self.num_actions = len(self.env.getActionSet()) # 1\n",
    "        self.agent = Agent(self.num_states, self.num_actions)\n",
    "    \n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10) # 10試行分の成功したstep数を格納し、平均ステップ数を出力に利用\n",
    "        episode_final = False # 最後の試行フラグ\n",
    "        reward_per_epoch = []\n",
    "        lifetime_per_epoch = []\n",
    "        \n",
    "        for episode in range(NUM_EPISODES): # 試行回数分繰り返す\n",
    "            \n",
    "            self.env.reset_game() # 環境の初期化\n",
    "            observation = self.game.getGameState() # 観測をそのまま状態sとして使用\n",
    "            state = observation\n",
    "            state = np.array(list(self.get_relative_state(state)))\n",
    "            state = torch.from_numpy(state).type(torch.FloatTensor) # numpy変数をPyTorchのテンソルに変換\n",
    "            # FloatTensor size 4 を size 1x4に変換\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            \n",
    "            # record frame\n",
    "            frames = [self.env.getScreenRGB()]\n",
    "            \n",
    "            cum_reward = 0  # このエピソードにおける累積報酬の和\n",
    "            t = 0 #  time-step数\n",
    "            step = 0 # episode数\n",
    "            \n",
    "            if episode % 15 == 0:\n",
    "                self.agent.update_target_model()\n",
    "            \n",
    "            \n",
    "            while not self.env.game_over():\n",
    "                step += 1\n",
    "                \n",
    "                action = self.agent.get_action(state, episode) # 行動を求める\n",
    "                # 出力されたactionをゲームに反映し、返り値に報酬を得る\n",
    "                rew = self.env.act(self.env.getActionSet()[action])\n",
    "                t += 1\n",
    "                observation_next = self.game.getGameState() \n",
    "                done = self.game.game_over()\n",
    "                \n",
    "                frames.append(self.env.getScreenRGB())\n",
    "                \n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "                    \n",
    "                    # 罰則を与える\n",
    "                    reward = torch.FloatTensor([-1.0])\n",
    "                    \n",
    "                else:\n",
    "                    if rew > 0:\n",
    "                        reward = torch.FloatTensor([1.0])\n",
    "                    else:\n",
    "                        #reward = torch.FloatTensor([0.1])  # 普段は報酬0.1\n",
    "                        reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    \n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    state_next = np.array(list(self.get_relative_state(state_next)))\n",
    "                    state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "                    \n",
    "                # 1 time-stepにおける報酬和\n",
    "                cum_reward += rew\n",
    "                \n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Q-networkを更新する\n",
    "                self.agent.update_q_network()\n",
    "\n",
    "                # 観測の更新\n",
    "                state = state_next\n",
    "                \n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                        episode, step + 1, episode_10_list.mean()))\n",
    "                    reward_per_epoch.append(cum_reward)\n",
    "                    lifetime_per_epoch.append(step+1)\n",
    "                    break\n",
    "                    \n",
    "            if episode_final is True:\n",
    "                # 動画の保存と描画\n",
    "                display_frames_as_gif(frames)\n",
    "                break\n",
    "                    \n",
    "            # 50エピソード毎にlogを出力\n",
    "            if episode % PRINT_EVERY_EPISODE == 0:\n",
    "                print(\"Episode %d finished after %f time steps\" % (episode, t))\n",
    "                print(\"cumulated reward: %f\" % cum_reward)\n",
    "                \n",
    "\n",
    "            # 100エピソード毎にアニメーションを作成\n",
    "            if episode % SHOW_GIF_EVERY_EPISODE == 0:\n",
    "                print(\"len frames:\", len(frames))\n",
    "                display_frames_as_gif(frames)\n",
    "                continue\n",
    "            \n",
    "            # 2000タイムステップ以上続いたアニメーションを作成\n",
    "            if step > 2000:\n",
    "                print(\"len frames:\", len(frames))\n",
    "                display_frames_as_gif(frames)\n",
    "                \n",
    "        # グラフの作成\n",
    "        make_graph(reward_per_epoch, lifetime_per_epoch)\n",
    "    \n",
    "    bucket_range_per_feature = {\n",
    "        'next_next_pipe_bottom_y': 40,\n",
    "        'next_next_pipe_dist_to_player': 512,\n",
    "        'next_next_pipe_top_y': 40,\n",
    "        'next_pipe_bottom_y': 20,\n",
    "        'next_pipe_dist_to_player': 20,\n",
    "        'next_pipe_top_y': 20,\n",
    "        'player_vel': 4,\n",
    "        'player_y': 16\n",
    "    }\n",
    "    \n",
    "    def get_relative_state(self, state):\n",
    "        # パイプの絶対位置の代わりに相対位置を使用する\n",
    "        state = copy.deepcopy(state)\n",
    "        state['next_next_pipe_bottom_y'] -= state['player_y']\n",
    "        state['next_next_pipe_top_y'] -= state['player_y']\n",
    "        state['next_pipe_bottom_y'] -= state['player_y']\n",
    "        state['next_pipe_top_y'] -= state['player_y']\n",
    "\n",
    "        # アルファベット順に並び替える\n",
    "        state_key = [k for k, v in sorted(state.items())]\n",
    "\n",
    "        # 相対位置を返す\n",
    "        state_idx = []\n",
    "        for key in state_key:\n",
    "            state_idx.append(int(state[key] / self.bucket_range_per_feature[key]))\n",
    "        return tuple(state_idx)\n",
    "\n",
    "    \n",
    "    def save_model():\n",
    "        torch.save(agent.brain.model.state_dict(), 'weight.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mainクラス\n",
    "flappy_env = Environment()\n",
    "flappy_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(flappy_env.agent.brain.model.state_dict(), 'weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
